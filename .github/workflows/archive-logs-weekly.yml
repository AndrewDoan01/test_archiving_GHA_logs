name: Weekly Batch Log Archive (fixed)

on:
  schedule:
    - cron: "0 2 * * 0" # Every Sunday 02:00 UTC
  workflow_dispatch:
    inputs:
      days_back:
        description: "Number of days to archive"
        required: false
        default: "7"

permissions:
  contents: write
  actions: read

jobs:
  batch-archive:
    runs-on: ubuntu-latest

    steps:
      - name: Checkout repository
        uses: actions/checkout@v4

      - name: Batch download logs
        uses: actions/github-script@v7
        id: download
        with:
          github-token: ${{ secrets.GITHUB_TOKEN }}
          script: |
            const fs = require('fs');
            const path = require('path');
                      const _core = require('@actions/core');

            const daysBack = parseInt('${{ github.event.inputs.days_back }}' || '7');
            const cutoffDate = new Date();
            cutoffDate.setDate(cutoffDate.getDate() - daysBack);

            const logsDir = 'batch-logs';
            if (!fs.existsSync(logsDir)) fs.mkdirSync(logsDir, { recursive: true });

            let page = 1;
            let totalArchived = 0;
            let totalSize = 0;
            const archiveIndex = [];

            while (page <= 5) {
              const runs = await github.rest.actions.listWorkflowRunsForRepo({
                owner: context.repo.owner,
                repo: context.repo.repo,
                per_page: 100,
                page,
              });

              if (!runs.data.workflow_runs.length) break;

              for (const run of runs.data.workflow_runs) {
                const runDate = new Date(run.created_at);
                if (runDate < cutoffDate) continue;

                try {
                  const logs = await github.rest.actions.downloadWorkflowRunLogs({
                    owner: context.repo.owner,
                    repo: context.repo.repo,
                    run_id: run.id,
                  });

                  if (!logs || !logs.data) continue;

                  const date = new Date(run.created_at);
                  const year = date.getFullYear();
                  const month = String(date.getMonth() + 1).padStart(2, '0');
                  const day = String(date.getDate()).padStart(2, '0');

                  const dateDir = path.join(logsDir, String(year), month, day);
                  if (!fs.existsSync(dateDir)) fs.mkdirSync(dateDir, { recursive: true });

                  const rawName = run.name || `workflow-${run.id}`;
                  const safeName = rawName.replace(/[^a-z0-9]/gi, '-').toLowerCase();
                  const ts = new Date(run.created_at).toISOString().replace(/[:.]/g, '-');
                  const filename = `${safeName}-${run.id}-${ts}.zip`;

                  fs.writeFileSync(path.join(dateDir, filename), Buffer.from(logs.data));

                  const metadata = {
                    workflow_name: run.name || rawName,
                    run_id: run.id,
                    run_number: run.run_number,
                    status: run.conclusion,
                    branch: run.head_branch,
                    commit: run.head_sha,
                    actor: run.actor?.login,
                    created_at: run.created_at,
                    size_bytes: logs.data ? logs.data.byteLength : 0,
                    file_path: `${year}/${month}/${day}/${filename}`,
                  };

                  fs.writeFileSync(path.join(dateDir, `${safeName}-${run.id}.json`), JSON.stringify(metadata, null, 2));
                  archiveIndex.push(metadata);
                  totalArchived++;
                  totalSize += logs.data.byteLength;

                } catch (err) {
                  console.log(`skip ${run.id}: ${err.message}`);
                }
              }

              page++;
            }

            fs.writeFileSync(path.join(logsDir, 'index.json'), JSON.stringify({
              archive_date: new Date().toISOString(),
              period: { from: cutoffDate.toISOString(), to: new Date().toISOString(), days: daysBack },
              statistics: { total_workflows: totalArchived, total_size_bytes: totalSize },
              workflows: archiveIndex,
            }, null, 2));

                      _core.setOutput('total_archived', totalArchived);
                      _core.setOutput('total_size_mb', (totalSize / 1024 / 1024).toFixed(2));

      - name: Create archive
        id: compress
        run: |
          ARCHIVE_DATE=$(date +%Y-%m-%d)
          ARCHIVE_NAME="logs-archive-${ARCHIVE_DATE}.tar.gz"
          tar -czf "${ARCHIVE_NAME}" batch-logs/
          sha256sum "${ARCHIVE_NAME}" > "${ARCHIVE_NAME}.sha256"
          echo "archive_name=${ARCHIVE_NAME}" >> $GITHUB_OUTPUT
          echo "archive_date=${ARCHIVE_DATE}" >> $GITHUB_OUTPUT
          SIZE=$(stat -c%s "${ARCHIVE_NAME}")
          SIZE_MB=$(echo "scale=2; ${SIZE} / 1024 / 1024" | bc)
          echo "archive_size_mb=${SIZE_MB}" >> $GITHUB_OUTPUT

      - name: Create release
        uses: softprops/action-gh-release@v1
        with:
          tag_name: logs-batch-${{ steps.compress.outputs.archive_date }}
          name: "Weekly Archive - ${{ steps.compress.outputs.archive_date }}"
          body: |
            Weekly batch archive generated by workflow.
          files: |
            ${{ steps.compress.outputs.archive_name }}
            ${{ steps.compress.outputs.archive_name }}.sha256
        env:
          GITHUB_TOKEN: ${{ secrets.GITHUB_TOKEN }}
